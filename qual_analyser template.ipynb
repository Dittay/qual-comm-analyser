{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Creating a qualitative comments analyser\n",
    "Reference: https://robertorocha.info/using-nlp-to-analyze-open-ended-responses-in-surveys/\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NLP and set large max length to go past spaCys default token limit\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 1850000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and check first 5 entries\n",
    "\n",
    "df = pd.read_csv('...csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text pre-processing\n",
    "\n",
    "# Function works on specific column in wider dataframe\n",
    "\n",
    "# To have it work on a dataframe that is only one column, or generally any column, replace x = x with x.iloc[:, 0] = x.iloc[:, 0]\n",
    "\n",
    "def text_clean(x):\n",
    "    # Set to lower case\n",
    "    x.iloc[:, 0] = x.iloc[:, 0].str.lower()\n",
    "\n",
    "    # Remove one or more whitespace characters including other unicode ones\n",
    "    x.iloc[:, 0] = x.iloc[:, 0].str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "    # Remove set of special characters\n",
    "    remove_spec_chars = [\"!\",'\"',\"%\",\"&\",\"'\",\"(\",\")\",\"#\",\"*\",\"?\",\n",
    "                    \"+\",\",\",\"-\",\"/\",\":\",\";\",\"<\",\"=\",\">\",\n",
    "                    \"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\"`\",\"{\",\"|\",\"}\",\n",
    "                    \"~\",\"–\",\"’\", \"*\"]\n",
    "    \n",
    "    for char in remove_spec_chars:\n",
    "        x.iloc[:, 0] = x.iloc[:, 0].str.replace(char, ' ')\n",
    "\n",
    "    # Handle periods not part of abbreviations\n",
    "    # This pattern aims to remove periods that are not followed by a lowercase letter (common in abbreviations)\n",
    "    x.iloc[:, 0] = x.iloc[:, 0].str.replace(r'\\.(?![a-z])', ' ', regex=True)\n",
    "\n",
    "    # Remove single characters\n",
    "    x.iloc[:, 0] = x.iloc[:, 0].replace(r'\\b[a-zA-Z]\\b', ' ', regex=True)\n",
    "\n",
    "    # Remove extra spaces (trim) from boh ends\n",
    "    x.iloc[:, 0] = x.iloc[:, 0].str.strip()\n",
    "\n",
    "    # Remove double spacing\n",
    "    x.iloc[:, 0] = x.iloc[:, 0].replace(r' +', ' ', regex=True)\n",
    "\n",
    "    # Remove spaces --\n",
    "    x.iloc[:, 0] = x.iloc[:, 0].replace(r'--', '', regex=True)\n",
    "\n",
    "    return x\n",
    "\n",
    "# test responses - 2349 values\n",
    "clean_df = text_clean(df)\n",
    "\n",
    "clean_df.loc[100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join all responses into a single mega string\n",
    "\n",
    "all_text = clean_df.Comments.str.cat(sep = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create spaCy document with the mega strings\n",
    "Named entity recognize (NER) disabled in tutorial, e.g.\n",
    "doc = nlp(all_text, disable = ['ner'])\n",
    "but not disabling for my comments'''\n",
    "\n",
    "doc = nlp(all_text)\n",
    "\n",
    "''' This splits words and tags them with parts-of-speech, and recognises stop-words'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overll word frequency analysis for most common words that aren't stop words or punctuation marks\n",
    "\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "words = [token.lemma_ for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "word_freq = Counter(words)\n",
    "word_freq.most_common(50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern word selection\n",
    "\"\"\"\n",
    "ADJ - adjective\n",
    "ADP - adposition\n",
    "ADV - adverb\n",
    "AUX - auxiliary verb\n",
    "CCONJ - coordinating conjunction\n",
    "DET - determiner\n",
    "INTJ -interjection\n",
    "NOUN - noun\n",
    "NUM - numeral\n",
    "PART - particle\n",
    "PRON - pronoun\n",
    "PROPN - proper noun\n",
    "PUNCT - punctuation\n",
    "SCONJ - subordinating conunction\n",
    "IN - conjuction\n",
    "VERB - verb\n",
    "X - other\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_one = \"ADJ\"\n",
    "pattern_two = \"NOUN\"\n",
    "patt_match_phrase = pattern_one +\"_\" + pattern_two + \"_\" + \"PHRASE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern: adjective-noun\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'POS':pattern_one}, {'POS':pattern_two}]\n",
    "matcher.add(patt_match_phrase, [pattern])\n",
    "\n",
    "matches = matcher(doc, as_spans=True)\n",
    "phrases = []\n",
    "\n",
    "for span in matches:\n",
    "    phrases.append(span.text.lower())\n",
    "    phrase_freq = Counter(phrases)\n",
    "\n",
    "phrase_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find surrounding words for a given phrase - word immediately before and after\n",
    "top_phrases = [phrase for phrase, freq in phrase_freq.most_common(20)]\n",
    "\n",
    "# Function to find surrounding words for a given phrase\n",
    "def find_surrounding_words(doc, phrase):\n",
    "    phrase_matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "    phrase_patterns = [nlp.make_doc(phrase)]\n",
    "    phrase_matcher.add(\"PhrasePattern\", phrase_patterns)\n",
    "    \n",
    "    matches = phrase_matcher(doc)\n",
    "    surrounding_words = []\n",
    "    \n",
    "    for _, start, end in matches:\n",
    "        before_index = max(start - 1, 0)\n",
    "        after_index = min(end, len(doc) - 1)\n",
    "        \n",
    "        if before_index != start - 1 or after_index != end:  # Skip if before or after word is out of bounds\n",
    "            continue\n",
    "            \n",
    "        if not doc[before_index].is_punct and not doc[before_index].is_stop and not doc[after_index].is_punct and not doc[after_index].is_stop:\n",
    "            pair = (doc[before_index].lemma_.lower(), doc[after_index].lemma_.lower())\n",
    "            surrounding_words.append(pair)\n",
    "            \n",
    "    return Counter(surrounding_words)\n",
    "\n",
    "# Step 3: Find and display the 10 most common surrounding word pairs for each top phrase\n",
    "for phrase in top_phrases:\n",
    "    surrounding_word_freq = find_surrounding_words(doc, phrase)\n",
    "    print(f\"Phrase: '{phrase}' - Top 10 surrounding word pairs:\")\n",
    "    for pair, freq in surrounding_word_freq.most_common(10):\n",
    "        print(f\"{pair}: {freq}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find surrounding words for a given phrase - three words immediately before and after (just replace 5s with x number for x words)\n",
    "\n",
    "top_phrases = [phrase for phrase, freq in phrase_freq.most_common(20)]\n",
    "\n",
    "# Function to find three words before and three words after for a given phrase\n",
    "def find_surrounding_words(doc, phrase):\n",
    "    phrase_matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "    phrase_patterns = [nlp.make_doc(phrase)]\n",
    "    phrase_matcher.add(\"PhrasePattern\", phrase_patterns)\n",
    "    \n",
    "    matches = phrase_matcher(doc)\n",
    "    surrounding_words = []\n",
    "    \n",
    "    for _, start, end in matches:\n",
    "        # Adjust indices to capture three words before and three words after\n",
    "        before_start = max(start - 5, 0)\n",
    "        after_end = min(end + 5, len(doc))\n",
    "        \n",
    "        # Construct the pair with three words before and three words after the phrase\n",
    "        before_words = doc[before_start:start].text.lower() if start - 5 >= 0 else ''\n",
    "        after_words = doc[end:after_end].text.lower() if end + 5 <= len(doc) else ''\n",
    "        \n",
    "        if before_words and after_words:  # Ensure both before and after words are captured\n",
    "            pair = (before_words, after_words)\n",
    "            surrounding_words.append(pair)\n",
    "            \n",
    "    return Counter(surrounding_words)\n",
    "\n",
    "# Step 3: Find and display the 10 most common pairs of three words before and after for each top phrase\n",
    "for phrase in top_phrases:\n",
    "    surrounding_word_freq = find_surrounding_words(doc, phrase)\n",
    "    print(f\"Phrase: '{phrase}' - Top 10 pairs of three words before and after:\")\n",
    "    for pair, freq in surrounding_word_freq.most_common(10):\n",
    "        print(f\"{pair}: {freq}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_phrases = [phrase for phrase, freq in phrase_freq.most_common(20)]\n",
    "\n",
    "# Updated function to include document index\n",
    "def find_surrounding_words_with_index(df, phrase):\n",
    "    phrase_matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "    phrase_patterns = [nlp.make_doc(phrase)]\n",
    "    phrase_matcher.add(\"PhrasePattern\", phrase_patterns)\n",
    "    \n",
    "    surrounding_words_with_index = defaultdict(list)\n",
    "    \n",
    "    for idx, doc in df['Comments'].items():\n",
    "        doc = nlp(doc)\n",
    "        matches = phrase_matcher(doc)\n",
    "        \n",
    "        for _, start, end in matches:\n",
    "            before_start = max(start - 3, 0)\n",
    "            after_end = min(end + 3, len(doc))\n",
    "            \n",
    "            before_words = doc[before_start:start].text.lower() if start - 3 >= 0 else ''\n",
    "            after_words = doc[end:after_end].text.lower() if end + 3 <= len(doc) else ''\n",
    "            \n",
    "            if before_words and after_words:\n",
    "                pair = (before_words, after_words)\n",
    "                surrounding_words_with_index[pair].append(idx)  # Store index where phrase was found\n",
    "            \n",
    "    return surrounding_words_with_index\n",
    "\n",
    "# Finding and displaying results for each top phrase\n",
    "for phrase in top_phrases:\n",
    "    surrounding_words_with_indices = find_surrounding_words_with_index(df, phrase)\n",
    "    print(f\"Phrase: '{phrase}' - Contexts and entry numbers:\")\n",
    "    \n",
    "    for pair, indices in surrounding_words_with_indices.items():\n",
    "        # Displaying pair and list of indices (data entry numbers) where the pair was found\n",
    "        print(f\"{pair}: found in entries {indices}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern: verb-adjective\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'POS':'VERB'}, {'POS':'ADJ'}]\n",
    "matcher.add('VERB_ADJ_PHRASE', [pattern])\n",
    "\n",
    "matches = matcher(doc, as_spans=True)\n",
    "phrases = []\n",
    "\n",
    "for span in matches:\n",
    "    phrases.append(span.text.lower())\n",
    "    phrase_freq = Counter(phrases)\n",
    "\n",
    "phrase_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find surrounding words for a given phrase - word immediately before and after\n",
    "top_phrases = [phrase for phrase, freq in phrase_freq.most_common(20)]\n",
    "\n",
    "# Function to find surrounding words for a given phrase\n",
    "def find_surrounding_words(doc, phrase):\n",
    "    phrase_matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "    phrase_patterns = [nlp.make_doc(phrase)]\n",
    "    phrase_matcher.add(\"PhrasePattern\", phrase_patterns)\n",
    "    \n",
    "    matches = phrase_matcher(doc)\n",
    "    surrounding_words = []\n",
    "    \n",
    "    for _, start, end in matches:\n",
    "        before_index = max(start - 1, 0)\n",
    "        after_index = min(end, len(doc) - 1)\n",
    "        \n",
    "        if before_index != start - 1 or after_index != end:  # Skip if before or after word is out of bounds\n",
    "            continue\n",
    "            \n",
    "        if not doc[before_index].is_punct and not doc[before_index].is_stop and not doc[after_index].is_punct and not doc[after_index].is_stop:\n",
    "            pair = (doc[before_index].lemma_.lower(), doc[after_index].lemma_.lower())\n",
    "            surrounding_words.append(pair)\n",
    "            \n",
    "    return Counter(surrounding_words)\n",
    "\n",
    "# Step 3: Find and display the 10 most common surrounding word pairs for each top phrase\n",
    "for phrase in top_phrases:\n",
    "    surrounding_word_freq = find_surrounding_words(doc, phrase)\n",
    "    print(f\"Phrase: '{phrase}' - Top 10 surrounding word pairs:\")\n",
    "    for pair, freq in surrounding_word_freq.most_common(10):\n",
    "        print(f\"{pair}: {freq}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find surrounding words for a given phrase - three words immediately before and after (just replace 3s with 2s for 2 words)\n",
    "\n",
    "top_phrases = [phrase for phrase, freq in phrase_freq.most_common(20)]\n",
    "\n",
    "# Function to find three words before and three words after for a given phrase\n",
    "def find_surrounding_words(doc, phrase):\n",
    "    phrase_matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "    phrase_patterns = [nlp.make_doc(phrase)]\n",
    "    phrase_matcher.add(\"PhrasePattern\", phrase_patterns)\n",
    "    \n",
    "    matches = phrase_matcher(doc)\n",
    "    surrounding_words = []\n",
    "    \n",
    "    for _, start, end in matches:\n",
    "        # Adjust indices to capture three words before and three words after\n",
    "        before_start = max(start - 3, 0)\n",
    "        after_end = min(end + 3, len(doc))\n",
    "        \n",
    "        # Construct the pair with three words before and three words after the phrase\n",
    "        before_words = doc[before_start:start].text.lower() if start - 3 >= 0 else ''\n",
    "        after_words = doc[end:after_end].text.lower() if end + 3 <= len(doc) else ''\n",
    "        \n",
    "        if before_words and after_words:  # Ensure both before and after words are captured\n",
    "            pair = (before_words, after_words)\n",
    "            surrounding_words.append(pair)\n",
    "            \n",
    "    return Counter(surrounding_words)\n",
    "\n",
    "# Step 3: Find and display the 10 most common pairs of three words before and after for each top phrase\n",
    "for phrase in top_phrases:\n",
    "    surrounding_word_freq = find_surrounding_words(doc, phrase)\n",
    "    print(f\"Phrase: '{phrase}' - Top 10 pairs of three words before and after:\")\n",
    "    for pair, freq in surrounding_word_freq.most_common(10):\n",
    "        print(f\"{pair}: {freq}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern: noun-noun\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'POS':'NOUN'}, {'POS':'NOUN'}]\n",
    "matcher.add('NOUN_NOUN_PHRASE', [pattern])\n",
    "\n",
    "matches = matcher(doc, as_spans=True)\n",
    "phrases = []\n",
    "\n",
    "for span in matches:\n",
    "    phrases.append(span.text.lower())\n",
    "    phrase_freq = Counter(phrases)\n",
    "\n",
    "phrase_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find surrounding words for a given phrase - three words immediately before and after (just replace 3s with 2s for 2 words)\n",
    "\n",
    "top_phrases = [phrase for phrase, freq in phrase_freq.most_common(20)]\n",
    "\n",
    "# Function to find three words before and three words after for a given phrase\n",
    "def find_surrounding_words(doc, phrase):\n",
    "    phrase_matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "    phrase_patterns = [nlp.make_doc(phrase)]\n",
    "    phrase_matcher.add(\"PhrasePattern\", phrase_patterns)\n",
    "    \n",
    "    matches = phrase_matcher(doc)\n",
    "    surrounding_words = []\n",
    "    \n",
    "    for _, start, end in matches:\n",
    "        # Adjust indices to capture three words before and three words after\n",
    "        before_start = max(start - 3, 0)\n",
    "        after_end = min(end + 3, len(doc))\n",
    "        \n",
    "        # Construct the pair with three words before and three words after the phrase\n",
    "        before_words = doc[before_start:start].text.lower() if start - 3 >= 0 else ''\n",
    "        after_words = doc[end:after_end].text.lower() if end + 3 <= len(doc) else ''\n",
    "        \n",
    "        if before_words and after_words:  # Ensure both before and after words are captured\n",
    "            pair = (before_words, after_words)\n",
    "            surrounding_words.append(pair)\n",
    "            \n",
    "    return Counter(surrounding_words)\n",
    "\n",
    "# Step 3: Find and display the 10 most common pairs of three words before and after for each top phrase\n",
    "for phrase in top_phrases:\n",
    "    surrounding_word_freq = find_surrounding_words(doc, phrase)\n",
    "    print(f\"Phrase: '{phrase}' - Top 10 pairs of three words before and after:\")\n",
    "    for pair, freq in surrounding_word_freq.most_common(10):\n",
    "        print(f\"{pair}: {freq}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern: noun-verb\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'POS':'NOUN'}, {'POS':'VERB'}]\n",
    "matcher.add('NOUN_VERB_PHRASE', [pattern])\n",
    "\n",
    "matches = matcher(doc, as_spans=True)\n",
    "phrases = []\n",
    "\n",
    "for span in matches:\n",
    "    phrases.append(span.text.lower())\n",
    "    phrase_freq = Counter(phrases)\n",
    "\n",
    "phrase_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find surrounding words for a given phrase - three words immediately before and after (just replace 3s with 2s for 2 words)\n",
    "\n",
    "top_phrases = [phrase for phrase, freq in phrase_freq.most_common(20)]\n",
    "\n",
    "# Function to find three words before and three words after for a given phrase\n",
    "def find_surrounding_words(doc, phrase):\n",
    "    phrase_matcher = PhraseMatcher(nlp.vocab, attr='LOWER')\n",
    "    phrase_patterns = [nlp.make_doc(phrase)]\n",
    "    phrase_matcher.add(\"PhrasePattern\", phrase_patterns)\n",
    "    \n",
    "    matches = phrase_matcher(doc)\n",
    "    surrounding_words = []\n",
    "    \n",
    "    for _, start, end in matches:\n",
    "        # Adjust indices to capture three words before and three words after\n",
    "        before_start = max(start - 5, 0)\n",
    "        after_end = min(end + 5, len(doc))\n",
    "        \n",
    "        # Construct the pair with three words before and three words after the phrase\n",
    "        before_words = doc[before_start:start].text.lower() if start - 5 >= 0 else ''\n",
    "        after_words = doc[end:after_end].text.lower() if end + 5 <= len(doc) else ''\n",
    "        \n",
    "        if before_words and after_words:  # Ensure both before and after words are captured\n",
    "            pair = (before_words, after_words)\n",
    "            surrounding_words.append(pair)\n",
    "            \n",
    "    return Counter(surrounding_words)\n",
    "\n",
    "# Step 3: Find and display the 10 most common pairs of three words before and after for each top phrase\n",
    "for phrase in top_phrases:\n",
    "    surrounding_word_freq = find_surrounding_words(doc, phrase)\n",
    "    print(f\"Phrase: '{phrase}' - Top 10 pairs of three words before and after:\")\n",
    "    for pair, freq in surrounding_word_freq.most_common(10):\n",
    "        print(f\"{pair}: {freq}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern: noun-adjective\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'POS':'NOUN'}, {'POS':'ADJ'}]\n",
    "matcher.add('ADJ_PHRASE', [pattern])\n",
    "\n",
    "matches = matcher(doc, as_spans=True)\n",
    "phrases = []\n",
    "\n",
    "for span in matches:\n",
    "    phrases.append(span.text.lower())\n",
    "    phrase_freq = Counter(phrases)\n",
    "\n",
    "phrase_freq.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern: adjective-noun-verb\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'POS':'ADJ'}, {'POS':'NOUN'}, {'POS':'VERB'}]\n",
    "matcher.add('NOUN_ADV_ADJ_PHRASE', [pattern])\n",
    "\n",
    "matches = matcher(doc, as_spans=True)\n",
    "phrases = []\n",
    "\n",
    "for span in matches:\n",
    "    phrases.append(span.text.lower())\n",
    "    phrase_freq = Counter(phrases)\n",
    "\n",
    "phrase_freq.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pattern: noun-noun-adverb-adjective\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{'POS':'NOUN'}, {'POS':'NOUN'}, {'POS':'ADV'}, {'POS':'ADJ'}]\n",
    "matcher.add('NOUN_ADV_ADJ_PHRASE', [pattern])\n",
    "\n",
    "matches = matcher(doc, as_spans=True)\n",
    "phrases = []\n",
    "\n",
    "for span in matches:\n",
    "    phrases.append(span.text.lower())\n",
    "    phrase_freq = Counter(phrases)\n",
    "\n",
    "phrase_freq.most_common(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
